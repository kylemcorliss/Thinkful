{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "FORMAT = '%(asctime)-15s %(message)s'\n",
    "logging.basicConfig(format=FORMAT, level=logging.INFO)\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-25 16:57:21,296 Getting wikipedia links...\n",
      "2019-05-25 16:57:40,163 Clean wikipedia URLs...\n",
      "2019-05-25 16:57:40,172 Setting up dataframes...\n",
      "2019-05-25 16:57:40,210 Complete!\n"
     ]
    }
   ],
   "source": [
    "import get_wikipedia_links\n",
    "\n",
    "link_scrape = get_wikipedia_links.get_wiki_links()\n",
    "link_scrape.fit(2005,2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_csv('data/wikipedia_links.csv')\n",
    "df_wiki = df_wiki.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache a pickled file for results, run a crawler with multiple workers\n",
    "def crawl_raw_data(workers, function, urls):\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:       \n",
    "        return executor.map(function, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR while fetching and parsing ['https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Dirty_Sanchez_%28TV_series%29%23Dirty_Sanchez%3A_The_Movie/daily/2013010100/2019052600']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\kylem\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\mwviews\\api\\pageviews.py\", line 145, in article_views\n",
      "    'The pageview API returned nothing useful at: {}'.format(urls)\n",
      "Exception: The pageview API returned nothing useful at: ['https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Dirty_Sanchez_%28TV_series%29%23Dirty_Sanchez%3A_The_Movie/daily/2013010100/2019052600']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR while fetching and parsing ['https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Tony_n%27_Tina%27s_Wedding%23Film_adaptation/daily/2013010100/2019052600']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\kylem\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\mwviews\\api\\pageviews.py\", line 145, in article_views\n",
      "    'The pageview API returned nothing useful at: {}'.format(urls)\n",
      "Exception: The pageview API returned nothing useful at: ['https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Tony_n%27_Tina%27s_Wedding%23Film_adaptation/daily/2013010100/2019052600']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR while fetching and parsing ['https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/No%25C3%25ABlle_%282007_film%29/daily/2013010100/2019052600']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\kylem\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\mwviews\\api\\pageviews.py\", line 145, in article_views\n",
      "    'The pageview API returned nothing useful at: {}'.format(urls)\n",
      "Exception: The pageview API returned nothing useful at: ['https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/No%25C3%25ABlle_%282007_film%29/daily/2013010100/2019052600']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR while fetching and parsing ['https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/The_World_Made_Straight%23Film_adaptation/daily/2013010100/2019052600']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\kylem\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\mwviews\\api\\pageviews.py\", line 145, in article_views\n",
      "    'The pageview API returned nothing useful at: {}'.format(urls)\n",
      "Exception: The pageview API returned nothing useful at: ['https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/The_World_Made_Straight%23Film_adaptation/daily/2013010100/2019052600']\n"
     ]
    }
   ],
   "source": [
    "from wikipedia import get_wiki_links, get_wiki_pageviews\n",
    "\n",
    "wiki_links = []\n",
    "wiki_pageviews = []\n",
    "for url, title in zip(df_wiki.wiki_url, df_wiki.clean_wiki_url):\n",
    "    wiki_links.append(get_wiki_links(url))\n",
    "    wiki_pageviews.append(get_wiki_pageviews(title))\n",
    "\n",
    "df_wiki_links = pd.DataFrame(wiki_links).set_index(['wiki_url'])\n",
    "df_wiki_pageviews = pd.concat([\n",
    "    pd.DataFrame.from_dict(title_dict,orient='index') \n",
    "    for title_dict in wiki_pageviews], axis=1, sort=False)\n",
    "\n",
    "df_wiki_pageviews.to_csv('data/wiki_pageviews_data.csv', sep=',',index=True)\n",
    "df_wiki_links.to_csv('data/wiki_links_data.csv', sep=',', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bomojo import get_data, clean_df\n",
    "\n",
    "df_wiki_links_dropna = df_wiki_links.dropna(subset=['bomojo_link'])\n",
    "df_bomojo = pd.DataFrame(crawl_raw_data(5, get_data, df_wiki_links_dropna.bomojo_link))\n",
    "df_bomojo.index = df_wiki_links_dropna.index\n",
    "\n",
    "df_bomojo = clean_df(df_bomojo)\n",
    "df_bomojo.to_csv('data/bomojo_data.csv',sep=',',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rottentomatoes import get_score\n",
    "\n",
    "df_wiki_links_dropna = df_wiki_links.dropna(subset=['rottentomatoes_link'])\n",
    "df_rt = pd.DataFrame(crawl_raw_data(10, get_score, df_wiki_links_dropna.rottentomatoes_link))\n",
    "df_rt.index = df_wiki_links_dropna.index\n",
    "\n",
    "df_rt.to_csv('data/rottentomatoes_data.csv',sep=',',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metacritics import get_score\n",
    "\n",
    "df_wiki_links_dropna = df_wiki_links.dropna(subset=['metacritic_link'])\n",
    "df_mc = pd.DataFrame(crawl_raw_data(10, get_score, df_wiki_links_dropna.metacritic_link))\n",
    "df_mc.index = df_wiki_links_dropna.index\n",
    "    \n",
    "df_mc.to_csv('data/metacritic_data.csv',sep=',',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kylem\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from themoviedb import run_tmdb, json_dump\n",
    "\n",
    "df_wiki_links_dropna = df_wiki_links.dropna(subset=['imdb_link'])\n",
    "df_wiki_links_dropna['imdb_link'] = df_wiki_links_dropna.imdb_link.apply(lambda x: x.split('title/')[1].replace('/',''))\n",
    "df_tmdb = pd.DataFrame(crawl_raw_data(5, run_tmdb, df_wiki_links_dropna.imdb_link))\n",
    "df_tmdb.index = df_wiki_links_dropna.index\n",
    "\n",
    "json_dump(df_tmdb,'genres','credits','keywords','production_companies','production_countries','video_stats')\n",
    "df_tmdb.to_csv('data/themoviedb_data.csv', sep=',',index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
